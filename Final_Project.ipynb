{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4447 Final Project - Predicting Real Disasters from Tweets\n",
    "## Hafez Gharbiah, Tyler Christeson\n",
    "## data: https://www.kaggle.com/vbmokin/nlp-with-disaster-tweets-cleaning-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric + Guidelines\n",
    "1. Proper tagging of Github repository for final report as per deadlines (0.5 = 0.25 + 0.25 points)\n",
    "1. Dataset and motivation slide (1 points)\n",
    "    - How/why the dataset was collected and a description of the metadata of your dataset.\n",
    "1. Actual task definition/research question (2 points)\n",
    "    - What real-world problem are you trying to solve? What are the input and output of your analysis?\n",
    "1. Literature review (2 points)\n",
    "    - What other work has been done in this area, and how is your work novel compared to others?\n",
    "1. Quality of cleaning (6 points, 2 points each) \n",
    "    - Data cleaning and type conversion activity. Please share anything unusual you faced during this activity.\n",
    "    - What did you do about missing values and why? Handling missing values properly is very important.\n",
    "    - New feature/attribute creation and data summary statistics and interpretation.\n",
    "1. Visualization (8 points, 2 points each)\n",
    "    - Data visualization activity (box plot, bar plot, violin plot, and pairplot to see relationships and distribution, etc.).\n",
    "    - Describe anything you find in the data after each visualization.\n",
    "    - What data visualization helped you understand about data distribution.\n",
    "    - What you did about possible outlier as per data distribution visualization. (Did you confirm with your client whether it is actually an outlier or put a disclosure statement in your notebook if you decided to remove it?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The problem we're trying to solve is predicting whether a tweet is about a real disaster or not, which can be used to determine if emergency services need to be sent.\n",
    "\n",
    "- We have a collection of 10,000 tweets. The attributes of the dataset are a unique identifier for each tweet, text of the tweet, where the tweet was sent from, keywords that could be used to identify disasters, and whether or not it is about a real disaster (only on some of them).\n",
    "\n",
    "- Examples of records:\n",
    "    - \"Heard about # earthquake is different cities, stay safe everyone .\" \n",
    "    - \"Please like and share our new page for our Indoor Trampoline Park Aftershock opening this fall !\" \n",
    "    - \" nowplaying Alfons - Ablaze 2015 on Puls Radio pulsradio\" \n",
    "    - \"Coincidence Or # Curse ? Still # Unresolved Secrets From Past # accident\"\n",
    "\n",
    "- This is a noisy data set because the tweets are not all about disasters, and certain disaster keywords are used in contexts that are not disasters . For example, while \"ablaze\" is used in several real disaster tweets about ongoing fires, in the above example it is used as a song title. The same is true for many keywords, like \"accident\" and \"aftershock\" above.\n",
    "\n",
    "- Feature engineering can be used in this dataset to:\n",
    "    - extract years to see if we're tweeting about events that aren't current\n",
    "    - extract news network name to determine if the accident is being reported on or not\n",
    "    - topic modeling to extract relevant topics as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body%20bags          40\n",
       "armageddon           37\n",
       "harm                 37\n",
       "deluge               36\n",
       "wrecked              36\n",
       "                     ..\n",
       "oil%20spill           1\n",
       "outbreak              1\n",
       "typhoon               1\n",
       "suicide%20bombing     1\n",
       "suicide%20bomber      1\n",
       "Name: keyword, Length: 218, dtype: int64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf = pd.read_csv('train_data_cleaning.csv',index_col=0)\n",
    "testdf = pd.read_csv('test_data_cleaning.csv',index_col=0)\n",
    "traindf.keyword[traindf.target==0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality of cleaning (6 points, 2 points each)\n",
    "### Data cleaning and type conversion activity. Please share anything unusual you faced during this activity.\n",
    "### What did you do about missing values and why? Handling missing values properly is very important.\n",
    "### New feature/attribute creation and data summary statistics and interpretation. \n",
    "train_text = traindf.text\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "train_text = [nltk.word_tokenize(i) for i in train_text]\n",
    "train_text = [[w.lower() for w in train_text[i] if w not in stopwords] for i in range(len(train_text))]\n",
    "\n",
    "wnetl = WordNetLemmatizer()\n",
    "\n",
    "def nltk_tag_pos(tag):   \n",
    "    #adapted from https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "    if tag[0]=='J':\n",
    "        return wordnet.ADJ\n",
    "    elif tag[0]=='V':\n",
    "        return wordnet.VERB\n",
    "    elif tag[0]=='N':\n",
    "        return wordnet.NOUN\n",
    "    elif tag[0]=='R':\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return wordnet.NOUN #noun is default lemmatize POS\n",
    "    \n",
    "train_text_POS = [nltk.pos_tag(i) for i in train_text]    \n",
    "train_text = [[wnetl.lemmatize(i[0],nltk_tag_pos(i[1])) for i in j] for j in train_text_POS]\n",
    "train_text = [' '.join(train_text[i]) for i in range(len(train_text))]\n",
    "\n",
    "traindf.text = train_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag of words model\n",
    "cv = CountVectorizer(max_features=1000)\n",
    "x= cv.fit_transform(train_text).toarray()\n",
    "y = traindf.target.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1209   79]\n",
      " [ 435  561]]\n",
      "Model accuracy is: 0.7749562171628721\n",
      "Randomly assigning \"target\" status would result in accuracy of : 0.4296597924602653\n"
     ]
    }
   ],
   "source": [
    "#70:30 train-test split\n",
    "xtrain, xtest, ytrain, ytest = sklearn.model_selection.train_test_split(x,y,test_size=0.3,random_state=1234)\n",
    "\n",
    "import sklearn.naive_bayes\n",
    "classifier = sklearn.naive_bayes.GaussianNB()\n",
    "classifier.fit(xtrain,ytrain)\n",
    "\n",
    "ypred = classifier.predict(xtest)\n",
    "ypred\n",
    "\n",
    "confusionMatrix = sklearn.metrics.confusion_matrix(ytest,ypred)\n",
    "print(confusionMatrix)\n",
    "tn, fp, fn, tp = confusionMatrix.ravel()\n",
    "accuracy = (tp+tn)/(sum(sum(confusionMatrix)))\n",
    "print(f'Model accuracy is: {accuracy}')\n",
    "print(f'Randomly assigning \"target\" status would result in accuracy of : {traindf.target.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
